{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jman4162/rankcal/blob/main/examples/tutorial.ipynb)\n\n# rankcal Tutorial: Calibration for Ranking Systems\n\nThis tutorial demonstrates how to use **rankcal** to calibrate ranking scores and make better decisions. You'll learn:\n\n1. Why calibration matters for ranking\n2. How to detect and visualize miscalibration\n3. How to choose and apply the right calibrator\n4. Why top-k calibration is different from overall calibration\n5. How to use calibrated scores for decision-making\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Quick Start (TL;DR)\n\n```python\nfrom rankcal import IsotonicCalibrator, ece\n\n# 1. Fit calibrator on held-out data (NOT training data!)\ncalibrator = IsotonicCalibrator()\ncalibrator.fit(validation_scores, validation_labels)\n\n# 2. Apply to new data\ncalibrated = calibrator(test_scores)\n\n# 3. Verify improvement\nprint(f\"ECE before: {ece(test_scores, labels):.4f}\")\nprint(f\"ECE after:  {ece(calibrated, labels):.4f}\")\n```\n\nThat's it! Read on for the full story.\n\n---\n\n## Installation (Colab)\n\nIf you're running this in Google Colab, uncomment and run the cell below to install rankcal:"
  },
  {
   "cell_type": "code",
   "source": "# !pip install rankcal",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up beautiful plots\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "plt.rcParams[\"font.size\"] = 11\n",
    "plt.rcParams[\"axes.titlesize\"] = 13\n",
    "plt.rcParams[\"axes.labelsize\"] = 11\n",
    "\n",
    "# Color palette for consistency\n",
    "COLORS = {\n",
    "    \"uncalibrated\": \"#e74c3c\",  # red\n",
    "    \"calibrated\": \"#27ae60\",    # green\n",
    "    \"perfect\": \"#2c3e50\",       # dark gray\n",
    "    \"positive\": \"#3498db\",      # blue\n",
    "    \"negative\": \"#e67e22\",      # orange\n",
    "    \"highlight\": \"#9b59b6\",     # purple\n",
    "}\n",
    "\n",
    "import rankcal\n",
    "print(f\"rankcal version: {rankcal.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. What is Calibration and Why Does It Matter?\n",
    "\n",
    "A ranking model produces **scores** that rank items. But can you trust these scores as **probabilities**?\n",
    "\n",
    "**Calibration** means: if your model outputs 0.7 for many items, about 70% of them should actually be relevant.\n",
    "\n",
    "This matters when you need to:\n",
    "- Set a threshold (\"show items with score > 0.5\")\n",
    "- Estimate expected outcomes (\"how many relevant items in top 100?\")\n",
    "- Combine scores from multiple models\n",
    "\n",
    "Let's see what miscalibration looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankcal import generate_miscalibrated_data, generate_calibrated_data\n",
    "\n",
    "# Generate well-calibrated data\n",
    "calibrated_scores, calibrated_labels = generate_calibrated_data(n_samples=5000, seed=42)\n",
    "\n",
    "# Generate overconfident (miscalibrated) data - common in neural networks\n",
    "overconfident_scores, overconfident_labels = generate_miscalibrated_data(\n",
    "    n_samples=5000, temperature=0.5, seed=42  # temperature < 1 = overconfident\n",
    ")\n",
    "\n",
    "# Generate underconfident data\n",
    "underconfident_scores, underconfident_labels = generate_miscalibrated_data(\n",
    "    n_samples=5000, temperature=2.0, seed=42  # temperature > 1 = underconfident\n",
    ")\n",
    "\n",
    "print(\"Data generated!\")\n",
    "print(f\"  - Calibrated: {len(calibrated_scores)} samples\")\n",
    "print(f\"  - Overconfident: {len(overconfident_scores)} samples\")\n",
    "print(f\"  - Underconfident: {len(underconfident_scores)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reliability Diagrams: Visualizing Calibration\n",
    "\n",
    "A **reliability diagram** shows predicted confidence vs actual accuracy. Perfect calibration = diagonal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from rankcal.metrics.ece import calibration_error_per_bin\n\ndef to_np(tensor):\n    \"\"\"Convert tensor to numpy, handling gradients.\"\"\"\n    if hasattr(tensor, 'detach'):\n        return tensor.detach().numpy()\n    return tensor.numpy()\n\ndef plot_reliability_diagram(ax, scores, labels, title, color, n_bins=10):\n    \"\"\"Create a beautiful reliability diagram.\"\"\"\n    # Detach scores if they have gradients\n    if hasattr(scores, 'detach'):\n        scores = scores.detach()\n    \n    centers, accs, confs, counts = calibration_error_per_bin(scores, labels, n_bins=n_bins)\n    mask = counts > 0\n    \n    # Perfect calibration line\n    ax.plot([0, 1], [0, 1], '--', color=COLORS[\"perfect\"], linewidth=2, \n            label=\"Perfect calibration\", zorder=1)\n    \n    # Confidence bars\n    bar_width = 0.08\n    bars = ax.bar(to_np(confs[mask]), to_np(accs[mask]), width=bar_width,\n                  color=color, alpha=0.8, edgecolor=\"white\", linewidth=1.5, zorder=2)\n    \n    # Gap visualization (shaded area showing miscalibration)\n    for conf, acc in zip(to_np(confs[mask]), to_np(accs[mask])):\n        if acc != conf:\n            ax.fill_between([conf - bar_width/2, conf + bar_width/2], \n                          [min(acc, conf), min(acc, conf)],\n                          [max(acc, conf), max(acc, conf)],\n                          color=\"red\", alpha=0.2, zorder=1)\n    \n    ax.set_xlabel(\"Mean Predicted Probability\")\n    ax.set_ylabel(\"Fraction of Positives\")\n    ax.set_title(title, fontweight=\"bold\")\n    ax.set_xlim(-0.02, 1.02)\n    ax.set_ylim(-0.02, 1.02)\n    ax.legend(loc=\"upper left\", framealpha=0.9)\n    ax.set_aspect(\"equal\")\n\n# Create comparison plot\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nplot_reliability_diagram(axes[0], calibrated_scores, calibrated_labels, \n                        \"Well-Calibrated\", COLORS[\"calibrated\"])\nplot_reliability_diagram(axes[1], overconfident_scores, overconfident_labels,\n                        \"Overconfident\\n(predictions too extreme)\", COLORS[\"uncalibrated\"])\nplot_reliability_diagram(axes[2], underconfident_scores, underconfident_labels,\n                        \"Underconfident\\n(predictions too moderate)\", COLORS[\"highlight\"])\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Calibration with ECE\n",
    "\n",
    "**Expected Calibration Error (ECE)** quantifies miscalibration as a single number (lower = better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankcal import ece, adaptive_ece, mce\n",
    "\n",
    "datasets = [\n",
    "    (\"Well-Calibrated\", calibrated_scores, calibrated_labels),\n",
    "    (\"Overconfident\", overconfident_scores, overconfident_labels),\n",
    "    (\"Underconfident\", underconfident_scores, underconfident_labels),\n",
    "]\n",
    "\n",
    "print(\"Calibration Metrics Comparison\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Dataset':<18} {'ECE':>10} {'Adaptive ECE':>14} {'MCE':>10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for name, scores, labels in datasets:\n",
    "    ece_val = ece(scores, labels).item()\n",
    "    aece_val = adaptive_ece(scores, labels).item()\n",
    "    mce_val = mce(scores, labels).item()\n",
    "    print(f\"{name:<18} {ece_val:>10.4f} {aece_val:>14.4f} {mce_val:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Fixing Miscalibration: Comparing Calibrators\n",
    "\n",
    "rankcal provides 4 calibrators with different tradeoffs:\n",
    "\n",
    "| Calibrator | Differentiable | Parameters | Best For |\n",
    "|------------|----------------|------------|----------|\n",
    "| `IsotonicCalibrator` | No | 0 | Post-hoc, production |\n",
    "| `TemperatureScaling` | Yes | 1 | Simple over/underconfidence |\n",
    "| `PiecewiseLinearCalibrator` | Yes | 10-20 | Moderate miscalibration |\n",
    "| `MonotonicNNCalibrator` | Yes | ~100+ | Complex patterns |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankcal import (\n",
    "    IsotonicCalibrator,\n",
    "    TemperatureScaling,\n",
    "    PiecewiseLinearCalibrator,\n",
    "    MonotonicNNCalibrator,\n",
    ")\n",
    "\n",
    "# Use overconfident data - split into train/test\n",
    "n_train = 3000\n",
    "train_scores, train_labels = overconfident_scores[:n_train], overconfident_labels[:n_train]\n",
    "test_scores, test_labels = overconfident_scores[n_train:], overconfident_labels[n_train:]\n",
    "\n",
    "# Fit all calibrators\n",
    "calibrators = {\n",
    "    \"Isotonic\": IsotonicCalibrator(),\n",
    "    \"Temperature\": TemperatureScaling(),\n",
    "    \"Piecewise Linear\": PiecewiseLinearCalibrator(n_knots=10),\n",
    "    \"Monotonic NN\": MonotonicNNCalibrator(hidden_dims=(16, 16)),\n",
    "}\n",
    "\n",
    "calibrated_outputs = {}\n",
    "for name, cal in calibrators.items():\n",
    "    cal.fit(train_scores, train_labels)\n",
    "    calibrated_outputs[name] = cal(test_scores)\n",
    "    \n",
    "print(\"All calibrators fitted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Original uncalibrated\n",
    "plot_reliability_diagram(axes[0], test_scores, test_labels,\n",
    "                        f\"Uncalibrated\\nECE = {ece(test_scores, test_labels):.4f}\",\n",
    "                        COLORS[\"uncalibrated\"])\n",
    "\n",
    "# Each calibrator\n",
    "cal_colors = [COLORS[\"calibrated\"], COLORS[\"positive\"], COLORS[\"highlight\"], COLORS[\"negative\"]]\n",
    "for i, (name, cal_scores) in enumerate(calibrated_outputs.items()):\n",
    "    ece_val = ece(cal_scores, test_labels)\n",
    "    plot_reliability_diagram(axes[i+1], cal_scores, test_labels,\n",
    "                            f\"{name}\\nECE = {ece_val:.4f}\",\n",
    "                            cal_colors[i])\n",
    "\n",
    "# Hide last subplot\n",
    "axes[5].axis(\"off\")\n",
    "\n",
    "# Add summary text\n",
    "summary_text = \"Summary:\\n\\n\"\n",
    "summary_text += f\"{'Method':<20} {'ECE':>8}\\n\"\n",
    "summary_text += \"-\" * 30 + \"\\n\"\n",
    "summary_text += f\"{'Uncalibrated':<20} {ece(test_scores, test_labels):>8.4f}\\n\"\n",
    "for name, cal_scores in calibrated_outputs.items():\n",
    "    summary_text += f\"{name:<20} {ece(cal_scores, test_labels):>8.4f}\\n\"\n",
    "\n",
    "axes[5].text(0.1, 0.5, summary_text, transform=axes[5].transAxes, \n",
    "            fontsize=12, fontfamily=\"monospace\", verticalalignment=\"center\")\n",
    "\n",
    "plt.suptitle(\"Calibration Comparison: Overconfident Model\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Calibration Functions\n",
    "\n",
    "Each calibrator learns a different transformation from raw scores to calibrated probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot calibration functions\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Input range\n",
    "x = torch.linspace(0.01, 0.99, 200)\n",
    "\n",
    "# Identity line\n",
    "ax.plot(x.numpy(), x.numpy(), '--', color=COLORS[\"perfect\"], linewidth=2,\n",
    "        label=\"No calibration (identity)\", zorder=1)\n",
    "\n",
    "# Plot each calibrator's function\n",
    "for (name, cal), color in zip(calibrators.items(), cal_colors):\n",
    "    y = cal(x).detach()\n",
    "    ax.plot(x.numpy(), y.numpy(), linewidth=2.5, label=name, color=color)\n",
    "\n",
    "ax.set_xlabel(\"Raw Score\", fontsize=12)\n",
    "ax.set_ylabel(\"Calibrated Probability\", fontsize=12)\n",
    "ax.set_title(\"Learned Calibration Functions\", fontsize=14, fontweight=\"bold\")\n",
    "ax.legend(loc=\"lower right\", fontsize=11)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_aspect(\"equal\")\n",
    "\n",
    "# Add annotation\n",
    "ax.annotate(\"Overconfident model:\\nhigh scores pulled down\",\n",
    "           xy=(0.85, 0.6), fontsize=10, ha=\"center\",\n",
    "           bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"wheat\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. The Key Insight: Top-k Calibration\n",
    "\n",
    "**In ranking, you often only care about the top items.** A model might be well-calibrated overall but poorly calibrated in the top-k where decisions happen.\n",
    "\n",
    "rankcal provides `ece_at_k` to measure calibration at specific positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankcal import ece_at_k, adaptive_ece_at_k, mce_at_k\n",
    "\n",
    "# Use isotonic calibration\n",
    "iso_calibrated = calibrated_outputs[\"Isotonic\"]\n",
    "\n",
    "# Measure ECE at different k values\n",
    "k_values = [10, 25, 50, 100, 200, 500, 1000, len(test_scores)]\n",
    "\n",
    "uncal_ece = [ece_at_k(test_scores, test_labels, k=k).item() for k in k_values]\n",
    "cal_ece = [ece_at_k(iso_calibrated, test_labels, k=k).item() for k in k_values]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x_pos = range(len(k_values))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar([p - width/2 for p in x_pos], uncal_ece, width, \n",
    "               label=\"Uncalibrated\", color=COLORS[\"uncalibrated\"], alpha=0.8)\n",
    "bars2 = ax.bar([p + width/2 for p in x_pos], cal_ece, width,\n",
    "               label=\"Calibrated (Isotonic)\", color=COLORS[\"calibrated\"], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"k (position cutoff)\", fontsize=12)\n",
    "ax.set_ylabel(\"ECE@k\", fontsize=12)\n",
    "ax.set_title(\"Calibration Error at Different Ranking Depths\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([str(k) if k < len(test_scores) else \"all\" for k in k_values])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, max(uncal_ece) * 1.2)\n",
    "\n",
    "# Add improvement annotations\n",
    "for i, (u, c) in enumerate(zip(uncal_ece, cal_ece)):\n",
    "    if u > 0:\n",
    "        improvement = (u - c) / u * 100\n",
    "        ax.annotate(f\"{improvement:.0f}%↓\", xy=(i, max(u, c) + 0.01),\n",
    "                   ha=\"center\", fontsize=9, color=COLORS[\"calibrated\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-side: Overall vs Top-100 Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_topk_reliability(ax, scores, labels, k, title, color):\n    \"\"\"Plot reliability diagram for top-k items only.\"\"\"\n    # Detach if needed\n    if hasattr(scores, 'detach'):\n        scores = scores.detach()\n    \n    # Get top-k\n    _, top_k_idx = torch.topk(scores, k)\n    top_scores = scores[top_k_idx]\n    top_labels = labels[top_k_idx]\n    \n    plot_reliability_diagram(ax, top_scores, top_labels, title, color)\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 12))\n\n# Uncalibrated\nplot_reliability_diagram(axes[0, 0], test_scores, test_labels,\n                        f\"Uncalibrated - All\\nECE = {ece(test_scores, test_labels):.4f}\",\n                        COLORS[\"uncalibrated\"])\nplot_topk_reliability(axes[0, 1], test_scores, test_labels, 100,\n                     f\"Uncalibrated - Top 100\\nECE@100 = {ece_at_k(test_scores, test_labels, 100):.4f}\",\n                     COLORS[\"uncalibrated\"])\n\n# Calibrated\nplot_reliability_diagram(axes[1, 0], iso_calibrated, test_labels,\n                        f\"Calibrated - All\\nECE = {ece(iso_calibrated, test_labels):.4f}\",\n                        COLORS[\"calibrated\"])\nplot_topk_reliability(axes[1, 1], iso_calibrated, test_labels, 100,\n                     f\"Calibrated - Top 100\\nECE@100 = {ece_at_k(iso_calibrated, test_labels, 100):.4f}\",\n                     COLORS[\"calibrated\"])\n\nplt.suptitle(\"Why Top-k Calibration Matters\", fontsize=14, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. New in v0.2.0: Adaptive ECE and MCE\n",
    "\n",
    "Standard ECE uses equal-width bins, which can be problematic when scores aren't uniformly distributed.\n",
    "\n",
    "**Adaptive ECE** uses equal-mass (quantile) bins - each bin has the same number of samples.\n",
    "\n",
    "**MCE (Maximum Calibration Error)** reports the worst-case bin error, important for safety-critical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create skewed score distribution (common in practice)\n",
    "torch.manual_seed(42)\n",
    "# Most scores clustered near 0.8-0.9 (overconfident model)\n",
    "skewed_scores = torch.clamp(0.85 + 0.1 * torch.randn(2000), 0.01, 0.99)\n",
    "skewed_labels = (torch.rand(2000) < skewed_scores * 0.7).float()  # Actual rate lower\n",
    "\n",
    "# Visualize the score distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Score histogram\n",
    "ax = axes[0]\n",
    "ax.hist(skewed_scores.numpy(), bins=30, color=COLORS[\"positive\"], alpha=0.7, edgecolor=\"white\")\n",
    "ax.set_xlabel(\"Score\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Skewed Score Distribution\", fontweight=\"bold\")\n",
    "ax.axvline(x=skewed_scores.mean().item(), color=COLORS[\"uncalibrated\"], linestyle=\"--\", \n",
    "          label=f\"Mean: {skewed_scores.mean():.2f}\")\n",
    "ax.legend()\n",
    "\n",
    "# Standard ECE bins (equal-width)\n",
    "ax = axes[1]\n",
    "_, _, _, counts_std = calibration_error_per_bin(skewed_scores, skewed_labels, n_bins=10)\n",
    "bin_edges = torch.linspace(0, 1, 11)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "ax.bar(bin_centers.numpy(), counts_std.numpy(), width=0.09, color=COLORS[\"uncalibrated\"], \n",
    "       alpha=0.7, edgecolor=\"white\")\n",
    "ax.set_xlabel(\"Bin Center\")\n",
    "ax.set_ylabel(\"Samples per Bin\")\n",
    "ax.set_title(\"Standard ECE: Equal-Width Bins\", fontweight=\"bold\")\n",
    "ax.annotate(\"Many empty bins!\", xy=(0.2, 50), fontsize=10,\n",
    "           bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.8))\n",
    "\n",
    "# Adaptive ECE bins (equal-mass)\n",
    "ax = axes[2]\n",
    "# For visualization, show that adaptive bins have equal samples\n",
    "quantiles = torch.linspace(0, 1, 11)\n",
    "adaptive_boundaries = torch.quantile(skewed_scores, quantiles)\n",
    "adaptive_centers = (adaptive_boundaries[:-1] + adaptive_boundaries[1:]) / 2\n",
    "# Each bin has ~200 samples (2000/10)\n",
    "ax.bar(adaptive_centers.numpy(), [200]*10, width=0.05, color=COLORS[\"calibrated\"],\n",
    "       alpha=0.7, edgecolor=\"white\")\n",
    "ax.set_xlabel(\"Bin Center (data-driven)\")\n",
    "ax.set_ylabel(\"Samples per Bin\")\n",
    "ax.set_title(\"Adaptive ECE: Equal-Mass Bins\", fontweight=\"bold\")\n",
    "ax.annotate(\"All bins well-populated!\", xy=(0.75, 220), fontsize=10,\n",
    "           bbox=dict(boxstyle=\"round\", facecolor=\"lightgreen\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare metrics\n",
    "print(\"\\nMetrics on Skewed Distribution:\")\n",
    "print(f\"  Standard ECE:  {ece(skewed_scores, skewed_labels):.4f}\")\n",
    "print(f\"  Adaptive ECE:  {adaptive_ece(skewed_scores, skewed_labels):.4f}\")\n",
    "print(f\"  MCE (worst):   {mce(skewed_scores, skewed_labels):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCE: Finding Worst-Case Calibration\n",
    "\n",
    "MCE tells you the worst calibration error in any bin. This is crucial when you can't afford failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ECE vs MCE across calibrators\n",
    "metrics_data = []\n",
    "methods = [\"Uncalibrated\"] + list(calibrated_outputs.keys())\n",
    "all_outputs = {\"Uncalibrated\": test_scores, **calibrated_outputs}\n",
    "\n",
    "for method in methods:\n",
    "    scores = all_outputs[method]\n",
    "    metrics_data.append({\n",
    "        \"Method\": method,\n",
    "        \"ECE\": ece(scores, test_labels).item(),\n",
    "        \"MCE\": mce(scores, test_labels).item(),\n",
    "    })\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = range(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "ece_vals = [d[\"ECE\"] for d in metrics_data]\n",
    "mce_vals = [d[\"MCE\"] for d in metrics_data]\n",
    "\n",
    "bars1 = ax.bar([i - width/2 for i in x], ece_vals, width, label=\"ECE (average)\",\n",
    "               color=COLORS[\"positive\"], alpha=0.8)\n",
    "bars2 = ax.bar([i + width/2 for i in x], mce_vals, width, label=\"MCE (worst-case)\",\n",
    "               color=COLORS[\"negative\"], alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Method\")\n",
    "ax.set_ylabel(\"Calibration Error\")\n",
    "ax.set_title(\"ECE vs MCE: Average vs Worst-Case\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods, rotation=15, ha=\"right\")\n",
    "ax.legend()\n",
    "\n",
    "# Add \"MCE >= ECE always\" annotation\n",
    "ax.annotate(\"MCE ≥ ECE always\\n(max ≥ mean)\", xy=(4, 0.35), fontsize=10, ha=\"center\",\n",
    "           bbox=dict(boxstyle=\"round\", facecolor=\"lightyellow\", alpha=0.9))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Decision Analysis: Using Calibrated Scores\n",
    "\n",
    "The real power of calibration is enabling **optimal decisions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankcal import (\n",
    "    risk_coverage_curve,\n",
    "    utility_curve,\n",
    "    optimal_threshold,\n",
    "    utility_budget_curve,\n",
    ")\n",
    "\n",
    "# Use calibrated scores for decision analysis\n",
    "cal_scores = iso_calibrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk-Coverage Curve\n",
    "\n",
    "Shows the tradeoff: to reduce risk (error rate), you must reduce coverage (fraction shown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage, risk = risk_coverage_curve(cal_scores, test_labels, n_thresholds=200)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.fill_between(coverage.numpy(), risk.numpy(), alpha=0.3, color=COLORS[\"positive\"])\n",
    "ax.plot(coverage.numpy(), risk.numpy(), linewidth=2.5, color=COLORS[\"positive\"])\n",
    "\n",
    "ax.set_xlabel(\"Coverage (fraction of items shown)\", fontsize=12)\n",
    "ax.set_ylabel(\"Risk (error rate on shown items)\", fontsize=12)\n",
    "ax.set_title(\"Risk-Coverage Tradeoff\", fontsize=14, fontweight=\"bold\")\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Add operating points\n",
    "for target_cov in [0.2, 0.5, 0.8]:\n",
    "    idx = (coverage - target_cov).abs().argmin()\n",
    "    ax.plot(coverage[idx], risk[idx], 'o', markersize=10, color=COLORS[\"highlight\"])\n",
    "    ax.annotate(f\"  {coverage[idx]:.0%} coverage\\n  {risk[idx]:.1%} error\",\n",
    "               xy=(coverage[idx], risk[idx]), fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Optimization\n",
    "\n",
    "Find the optimal threshold given a cost/benefit tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Different cost scenarios\n",
    "scenarios = [\n",
    "    (1, 1, \"Equal (b=1, c=1)\"),\n",
    "    (1, 3, \"Conservative (b=1, c=3)\"),\n",
    "    (3, 1, \"Aggressive (b=3, c=1)\"),\n",
    "]\n",
    "\n",
    "# Utility curves\n",
    "ax = axes[0]\n",
    "colors = [COLORS[\"positive\"], COLORS[\"uncalibrated\"], COLORS[\"calibrated\"]]\n",
    "\n",
    "for (benefit, cost, label), color in zip(scenarios, colors):\n",
    "    thresholds, utility = utility_curve(cal_scores, test_labels, benefit=benefit, cost=cost)\n",
    "    ax.plot(thresholds.numpy(), utility.numpy(), linewidth=2, label=label, color=color)\n",
    "    \n",
    "    # Mark optimal\n",
    "    opt_thresh, opt_util = optimal_threshold(cal_scores, test_labels, benefit=benefit, cost=cost)\n",
    "    ax.plot(opt_thresh.item(), opt_util.item(), 'o', markersize=10, color=color)\n",
    "\n",
    "ax.set_xlabel(\"Threshold\", fontsize=12)\n",
    "ax.set_ylabel(\"Utility\", fontsize=12)\n",
    "ax.set_title(\"Utility vs Threshold\", fontsize=13, fontweight=\"bold\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Utility vs budget\n",
    "ax = axes[1]\n",
    "budgets, utilities = utility_budget_curve(cal_scores, test_labels, max_budget=len(test_labels))\n",
    "\n",
    "ax.fill_between(budgets.numpy(), utilities.numpy(), alpha=0.3, color=COLORS[\"calibrated\"])\n",
    "ax.plot(budgets.numpy(), utilities.numpy(), linewidth=2, color=COLORS[\"calibrated\"])\n",
    "\n",
    "# Find break-even and optimal\n",
    "breakeven_idx = (utilities > 0).nonzero()[0][-1].item() if (utilities > 0).any() else 0\n",
    "optimal_idx = utilities.argmax().item()\n",
    "\n",
    "ax.axvline(x=budgets[optimal_idx], color=COLORS[\"highlight\"], linestyle=\"--\", alpha=0.7)\n",
    "ax.plot(budgets[optimal_idx], utilities[optimal_idx], 'o', markersize=12, \n",
    "        color=COLORS[\"highlight\"], label=f\"Optimal: {budgets[optimal_idx]} items\")\n",
    "\n",
    "ax.set_xlabel(\"Budget (# items to review)\", fontsize=12)\n",
    "ax.set_ylabel(\"Utility\", fontsize=12)\n",
    "ax.set_title(\"Utility vs Budget\", fontsize=13, fontweight=\"bold\")\n",
    "ax.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Complete Workflow Example\n",
    "\n",
    "Here's the recommended pattern for using rankcal in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simulate a real workflow\ntorch.manual_seed(123)\n\n# 1. Your model produces raw scores (simulated here)\nn_total = 10000\nraw_scores = torch.sigmoid(torch.randn(n_total) * 1.5)  # Overconfident model\ntrue_labels = (torch.rand(n_total) < raw_scores * 0.8).float()  # True relevance\n\n# 2. Split: 70% train, 15% calibration, 15% test\nn_train = int(0.70 * n_total)\nn_cal = int(0.15 * n_total)\n\nworkflow_train_scores = raw_scores[:n_train]\nworkflow_train_labels = true_labels[:n_train]\n\nworkflow_cal_scores = raw_scores[n_train:n_train+n_cal]\nworkflow_cal_labels = true_labels[n_train:n_train+n_cal]\n\nworkflow_test_scores = raw_scores[n_train+n_cal:]\nworkflow_test_labels = true_labels[n_train+n_cal:]\n\nprint(f\"Split: {n_train} train / {n_cal} calibration / {len(workflow_test_scores)} test\")\n\n# 3. Fit calibrator on calibration set (NOT training set!)\nworkflow_calibrator = IsotonicCalibrator()\nworkflow_calibrator.fit(workflow_cal_scores, workflow_cal_labels)\n\n# 4. Apply to test set\nworkflow_calibrated = workflow_calibrator(workflow_test_scores)\n\n# 5. Evaluate\nprint(f\"\\nTest Set Results:\")\nprint(f\"  ECE (before):     {ece(workflow_test_scores, workflow_test_labels):.4f}\")\nprint(f\"  ECE (after):      {ece(workflow_calibrated, workflow_test_labels):.4f}\")\nprint(f\"  ECE@50 (before):  {ece_at_k(workflow_test_scores, workflow_test_labels, k=50):.4f}\")\nprint(f\"  ECE@50 (after):   {ece_at_k(workflow_calibrated, workflow_test_labels, k=50):.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final visualization\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Before\nplot_reliability_diagram(axes[0], workflow_test_scores, workflow_test_labels,\n                        f\"Before Calibration\\nECE = {ece(workflow_test_scores, workflow_test_labels):.4f}\",\n                        COLORS[\"uncalibrated\"])\n\n# After\nplot_reliability_diagram(axes[1], workflow_calibrated, workflow_test_labels,\n                        f\"After Calibration\\nECE = {ece(workflow_calibrated, workflow_test_labels):.4f}\",\n                        COLORS[\"calibrated\"])\n\n# Score distributions\nax = axes[2]\nax.hist(workflow_test_scores.numpy(), bins=30, alpha=0.5, label=\"Before\", \n        color=COLORS[\"uncalibrated\"], density=True)\nax.hist(workflow_calibrated.detach().numpy(), bins=30, alpha=0.5, label=\"After\",\n        color=COLORS[\"calibrated\"], density=True)\nax.set_xlabel(\"Score\")\nax.set_ylabel(\"Density\")\nax.set_title(\"Score Distribution Shift\", fontweight=\"bold\")\nax.legend()\n\nplt.suptitle(\"Complete Calibration Workflow Results\", fontsize=14, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 7. Bonus: Calibration Heatmap\n\nVisualize how calibration varies across both score ranges and ranking positions:"
  },
  {
   "cell_type": "code",
   "source": "def compute_calibration_heatmap(scores, labels, k_values, n_bins=5):\n    \"\"\"Compute calibration error for different k and score ranges.\"\"\"\n    # Detach if needed\n    if hasattr(scores, 'detach'):\n        scores = scores.detach()\n    \n    heatmap = np.zeros((len(k_values), n_bins))\n    \n    bin_edges = torch.linspace(0, 1, n_bins + 1)\n    \n    for i, k in enumerate(k_values):\n        # Get top-k\n        _, top_k_idx = torch.topk(scores, min(k, len(scores)))\n        top_scores = scores[top_k_idx]\n        top_labels = labels[top_k_idx]\n        \n        # Compute per-bin error\n        for j in range(n_bins):\n            mask = (top_scores >= bin_edges[j]) & (top_scores < bin_edges[j+1])\n            if mask.sum() > 0:\n                conf = top_scores[mask].mean()\n                acc = top_labels[mask].mean()\n                heatmap[i, j] = abs(conf - acc).item()\n            else:\n                heatmap[i, j] = np.nan\n                \n    return heatmap\n\n# Compare uncalibrated vs calibrated\nk_values = [50, 100, 200, 500, 1000, 2000]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Uncalibrated heatmap\nheatmap_uncal = compute_calibration_heatmap(test_scores, test_labels, k_values)\nim1 = axes[0].imshow(heatmap_uncal, cmap=\"RdYlGn_r\", aspect=\"auto\", vmin=0, vmax=0.4)\naxes[0].set_yticks(range(len(k_values)))\naxes[0].set_yticklabels(k_values)\naxes[0].set_xticks(range(5))\naxes[0].set_xticklabels([\"0-0.2\", \"0.2-0.4\", \"0.4-0.6\", \"0.6-0.8\", \"0.8-1.0\"])\naxes[0].set_ylabel(\"Top-k\")\naxes[0].set_xlabel(\"Score Range\")\naxes[0].set_title(\"Uncalibrated: Calibration Error by Position & Score\", fontweight=\"bold\")\n\n# Add values\nfor i in range(len(k_values)):\n    for j in range(5):\n        val = heatmap_uncal[i, j]\n        if not np.isnan(val):\n            axes[0].text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", \n                        color=\"white\" if val > 0.2 else \"black\", fontsize=9)\n\n# Calibrated heatmap\nheatmap_cal = compute_calibration_heatmap(iso_calibrated, test_labels, k_values)\nim2 = axes[1].imshow(heatmap_cal, cmap=\"RdYlGn_r\", aspect=\"auto\", vmin=0, vmax=0.4)\naxes[1].set_yticks(range(len(k_values)))\naxes[1].set_yticklabels(k_values)\naxes[1].set_xticks(range(5))\naxes[1].set_xticklabels([\"0-0.2\", \"0.2-0.4\", \"0.4-0.6\", \"0.6-0.8\", \"0.8-1.0\"])\naxes[1].set_ylabel(\"Top-k\")\naxes[1].set_xlabel(\"Score Range\")\naxes[1].set_title(\"Calibrated: Calibration Error by Position & Score\", fontweight=\"bold\")\n\n# Add values\nfor i in range(len(k_values)):\n    for j in range(5):\n        val = heatmap_cal[i, j]\n        if not np.isnan(val):\n            axes[1].text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\",\n                        color=\"white\" if val > 0.2 else \"black\", fontsize=9)\n\n# Colorbar\nfig.colorbar(im2, ax=axes, label=\"Calibration Error\", shrink=0.8)\n\nplt.suptitle(\"Where is Miscalibration Worst?\", fontsize=14, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## 8. Production Monitoring Dashboard\n\nTrack calibration metrics over time or across user segments:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Simulate monitoring data over time (e.g., daily batches)\ndef simulate_monitoring_data(n_days=14):\n    \"\"\"Simulate calibration drift over time.\"\"\"\n    np.random.seed(42)\n    data = []\n    \n    for day in range(n_days):\n        # Simulate gradual drift (calibration gets worse over time)\n        drift = day * 0.005\n        n_samples = np.random.randint(800, 1200)\n        \n        scores = torch.rand(n_samples)\n        # Add drift: model becomes overconfident over time\n        labels = (torch.rand(n_samples) < (scores - drift).clamp(0, 1)).float()\n        \n        data.append({\n            \"day\": day + 1,\n            \"ece\": ece(scores, labels).item(),\n            \"mce\": mce(scores, labels).item(),\n            \"ece_at_100\": ece_at_k(scores, labels, k=100).item(),\n            \"n_samples\": n_samples,\n        })\n    \n    return data\n\nmonitoring_data = simulate_monitoring_data()\n\n# Create monitoring dashboard\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\ndays = [d[\"day\"] for d in monitoring_data]\n\n# ECE over time\nax = axes[0, 0]\nece_vals = [d[\"ece\"] for d in monitoring_data]\nax.plot(days, ece_vals, 'o-', color=COLORS[\"positive\"], linewidth=2, markersize=8)\nax.axhline(y=0.05, color=COLORS[\"uncalibrated\"], linestyle=\"--\", alpha=0.7, label=\"Alert threshold\")\nax.fill_between(days, 0, ece_vals, alpha=0.2, color=COLORS[\"positive\"])\nax.set_xlabel(\"Day\")\nax.set_ylabel(\"ECE\")\nax.set_title(\"Daily ECE Trend\", fontweight=\"bold\")\nax.legend()\nax.set_ylim(0, max(ece_vals) * 1.2)\n\n# ECE vs MCE comparison\nax = axes[0, 1]\nmce_vals = [d[\"mce\"] for d in monitoring_data]\nax.plot(days, ece_vals, 'o-', color=COLORS[\"positive\"], linewidth=2, markersize=6, label=\"ECE (avg)\")\nax.plot(days, mce_vals, 's-', color=COLORS[\"negative\"], linewidth=2, markersize=6, label=\"MCE (worst)\")\nax.set_xlabel(\"Day\")\nax.set_ylabel(\"Calibration Error\")\nax.set_title(\"ECE vs MCE Over Time\", fontweight=\"bold\")\nax.legend()\n\n# ECE@100 (top-k monitoring)\nax = axes[1, 0]\nece_100 = [d[\"ece_at_100\"] for d in monitoring_data]\nax.bar(days, ece_100, color=COLORS[\"highlight\"], alpha=0.7, edgecolor=\"white\")\nax.axhline(y=0.10, color=COLORS[\"uncalibrated\"], linestyle=\"--\", alpha=0.7, label=\"Alert threshold\")\nax.set_xlabel(\"Day\")\nax.set_ylabel(\"ECE@100\")\nax.set_title(\"Top-100 Calibration (Where Decisions Happen)\", fontweight=\"bold\")\nax.legend()\n\n# Sample count\nax = axes[1, 1]\nsamples = [d[\"n_samples\"] for d in monitoring_data]\nax.bar(days, samples, color=COLORS[\"calibrated\"], alpha=0.7, edgecolor=\"white\")\nax.set_xlabel(\"Day\")\nax.set_ylabel(\"Samples\")\nax.set_title(\"Daily Sample Volume\", fontweight=\"bold\")\nax.axhline(y=np.mean(samples), color=\"gray\", linestyle=\"--\", alpha=0.7, label=f\"Avg: {np.mean(samples):.0f}\")\nax.legend()\n\nplt.suptitle(\"Calibration Monitoring Dashboard\", fontsize=14, fontweight=\"bold\", y=1.02)\nplt.tight_layout()\nplt.show()\n\n# Alert summary\nprint(\"\\n\" + \"=\"*50)\nprint(\"MONITORING SUMMARY\")\nprint(\"=\"*50)\nlatest = monitoring_data[-1]\nprint(f\"Latest ECE:     {latest['ece']:.4f}  {'⚠️  ALERT' if latest['ece'] > 0.05 else '✓'}\")\nprint(f\"Latest MCE:     {latest['mce']:.4f}  {'⚠️  ALERT' if latest['mce'] > 0.15 else '✓'}\")\nprint(f\"Latest ECE@100: {latest['ece_at_100']:.4f}  {'⚠️  ALERT' if latest['ece_at_100'] > 0.10 else '✓'}\")\nprint(f\"\\nTrend: ECE increased {(ece_vals[-1] - ece_vals[0])/ece_vals[0]*100:.1f}% over {len(days)} days\")\nif ece_vals[-1] > ece_vals[0] * 1.5:\n    print(\"⚠️  Consider recalibrating the model!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Summary\n\n### Key Takeaways\n\n1. **Calibration matters** when you use scores for decisions, not just ranking\n2. **Reliability diagrams** visualize calibration quality\n3. **ECE** measures average miscalibration; **MCE** measures worst-case\n4. **Top-k calibration** often differs from overall - use `ece_at_k`\n5. **Adaptive ECE** handles skewed score distributions better\n6. **Never calibrate on training data** - always use a held-out set\n7. **Monitor calibration** in production - it drifts over time!\n\n### Choosing a Calibrator\n\n```\nNeed differentiable? ─No──→ IsotonicCalibrator (recommended default)\n         │\n        Yes\n         │\n         ├─ Simple scaling needed? ──→ TemperatureScaling\n         │\n         └─ Complex pattern? ──→ PiecewiseLinearCalibrator or MonotonicNNCalibrator\n```\n\n### Metrics Cheat Sheet\n\n| Metric | Use When | Interpretation |\n|--------|----------|----------------|\n| `ece` | General calibration check | < 0.05 is good |\n| `ece_at_k` | Ranking systems | Focus on top-k where decisions happen |\n| `adaptive_ece` | Skewed score distributions | More robust than standard ECE |\n| `mce` | Safety-critical applications | Worst-case bin error |\n\n### Learn More\n\n- **Conceptual Guide**: `docs/guide.md`\n- **API Reference**: Docstrings for all functions\n- **Examples**: `examples/` directory\n\n---\n\n*Tutorial created for rankcal v0.2.0*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}